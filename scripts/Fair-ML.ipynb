{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>LSAT</th>\n",
       "      <th>UGPA</th>\n",
       "      <th>region_first</th>\n",
       "      <th>ZFYA</th>\n",
       "      <th>sander_index</th>\n",
       "      <th>first_pf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>GL</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.782738</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GL</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.735714</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>White</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>MS</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.670238</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NE</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.697024</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>GL</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>0.786310</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      race  sex  LSAT  UGPA region_first  ZFYA  sander_index   \n",
       "0           0     White    1  39.0   3.1           GL -0.98      0.782738  \\\n",
       "1           1     White    1  36.0   3.0           GL  0.09      0.735714   \n",
       "2           2     White    2  30.0   3.1           MS -0.35      0.670238   \n",
       "3           5  Hispanic    2  39.0   2.2           NE  0.58      0.697024   \n",
       "4           6     White    1  37.0   3.4           GL -1.26      0.786310   \n",
       "\n",
       "   first_pf  \n",
       "0       1.0  \n",
       "1       1.0  \n",
       "2       1.0  \n",
       "3       1.0  \n",
       "4       1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = 'Fair-ML-Causal-Inference'\n",
    "law_path = os.path.join('~', base_path, 'data', 'law_data.csv')\n",
    "data = pd.read_csv(law_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region_first\n",
       "NE    4302\n",
       "GL    3822\n",
       "FW    2904\n",
       "SE    2651\n",
       "MS    2346\n",
       "SC    2251\n",
       "Mt    1147\n",
       "NG    1133\n",
       "MW    1071\n",
       "NW     163\n",
       "PO       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data['region_first'].value_counts())\n",
    "data = data.loc[data['region_first'] != 'PO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21790 entries, 0 to 21790\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   race    21790 non-null  object \n",
      " 1   sex     21790 non-null  int64  \n",
      " 2   LSAT    21790 non-null  float64\n",
      " 3   UGPA    21790 non-null  float64\n",
      " 4   ZFYA    21790 non-null  float64\n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 1021.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cols_keep = ['race', 'sex', 'LSAT', 'UGPA', 'ZFYA'] \n",
    "law_data = data[cols_keep]\n",
    "law_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race\n",
       "White          18284\n",
       "Black           1282\n",
       "Asian            845\n",
       "Hispanic         488\n",
       "Mexican          389\n",
       "Other            293\n",
       "Puertorican      110\n",
       "Amerindian        99\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sex\n",
       "2    12253\n",
       "1     9537\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the distributions for our OHE data\n",
    "display(law_data['race'].value_counts(), law_data['sex'].value_counts())\n",
    "\n",
    "# convert sex to category\n",
    "law_data.loc[:,'sex'] = np.where(law_data['sex'] == 1, 'Female', 'Male')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/grad/khern045/.local/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/accounts/grad/khern045/.local/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# split the data first to avoid data leakage\n",
    "train, test = train_test_split(law_data, train_size=0.8, random_state=256)\n",
    "\n",
    "# explicity categories and their unique values\n",
    "categories = [('sex', list(law_data['sex'].unique())),\n",
    "              ('race', list(law_data['race'].unique()))]\n",
    "\n",
    "ohe_columns = [x[0] for x in categories]\n",
    "ohe_categories = [x[1] for x in categories]\n",
    "\n",
    "# initialize OHE\n",
    "enc = OneHotEncoder(sparse=False, categories=ohe_categories, )\n",
    "\n",
    "# fit and transform the train\n",
    "train_trans = pd.DataFrame(\n",
    "    enc.fit_transform(train[ohe_columns]),\n",
    "    columns = enc.get_feature_names_out(),\n",
    "    index = train.index\n",
    ")\n",
    "# concatenate transformed cols with non transformed\n",
    "train_trans = pd.concat([train.drop(ohe_columns, axis=1), train_trans], axis=1).reset_index(drop=True)\n",
    "train_trans.columns = [col.split('_')[1] if '_' in col else col for col in train_trans.columns]\n",
    "\n",
    "\n",
    "# apply same transformation to test\n",
    "test_trans = pd.DataFrame(\n",
    "    enc.fit_transform(test[ohe_columns]),\n",
    "    columns = enc.get_feature_names_out(),\n",
    "    index = test.index\n",
    ")\n",
    "test_trans = pd.concat([test.drop(ohe_columns, axis=1) ,test_trans], axis=1).reset_index(drop=True)\n",
    "test_trans.columns = [col.split('_')[1] if '_' in col else col for col in test_trans.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = train_trans.shape[0]\n",
    "n_test = test_trans.shape[0]\n",
    "\n",
    "train_trans['LSAT'] = train_trans['LSAT'].round()\n",
    "test_trans['LSAT'] = test_trans['LSAT'].round()\n",
    "\n",
    "X_train, y_train = torch.tensor(train_trans.drop(['ZFYA'], axis=1).values, dtype=torch.float32), torch.tensor(train_trans['ZFYA'], dtype=torch.float32) \n",
    "X_test, y_test = torch.tensor(test_trans.drop(['ZFYA'], axis=1).values, dtype=torch.float32), torch.tensor(test_trans['ZFYA'], dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "     def __init__(self, dataframe):\n",
    "          self.dataframe = dataframe\n",
    "      \n",
    "     def __len__(self):\n",
    "          return self.dataframe.shape[0]\n",
    "     \n",
    "     def __getitem__(self, idx):\n",
    "          x = torch.tensor(self.dataframe.drop(['ZFYA'], axis=1).loc[idx,:].values, dtype=torch.float32)\n",
    "          y = torch.tensor(self.dataframe.loc[idx, 'ZFYA'], dtype=torch.float32)\n",
    "          return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class inherits form a class called nn.Module\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    # initialization method for new class\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # first thing always do is call initialization method from the parent class, nn.module\n",
    "        super().__init__()\n",
    "\n",
    "        # fully connected linear layer\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # run the linear layer\n",
    "        output = self.fc1(x)\n",
    "\n",
    "        return output\n",
    "    \n",
    "# evaluate models performance\n",
    "def evaluate(model, X_train, y_test):\n",
    "    # Make predictions\n",
    "    with torch.no_grad(): # disable gradient computation\n",
    "        predictions = model(X_train).squeeze()\n",
    "\n",
    "    # Calculate RMSE\n",
    "    mse = torch.nn.functional.mse_loss(predictions, y_test)\n",
    "    rmse = np.sqrt(mse.item())\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def train(network, train_dataset, test_dataset, file_name_model, n_epochs=10, batch_size = 25):\n",
    "    assert isinstance(file_name_model, str), \"The filename is not a string\"\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(Dataset(train_dataset), batch_size = batch_size, shuffle=True)\n",
    "    \n",
    "    X_test = torch.tensor(test_dataset.drop(['ZFYA'], axis=1).values, dtype=torch.float32)\n",
    "    y_test = torch.tensor(test_dataset['ZFYA'], dtype=torch.float32)\n",
    "    \n",
    "    # Move network to GPU if available\n",
    "    network = network.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters())\n",
    "\n",
    "    # best validation score initialization \n",
    "    validation_score_best = float('inf')\n",
    "    train_losses = []\n",
    "    validation_scores = []\n",
    "\n",
    "    # train loop\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm(data_loader, leave=False):\n",
    "            # unpack batch\n",
    "            X, y = batch\n",
    "\n",
    "            # zero parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass to get input\n",
    "            # output is of shape [20,1] but we want of size [20] to compare \n",
    "            output = network(X).squeeze()\n",
    "\n",
    "            # calculate loss\n",
    "            loss = nn.MSELoss()(output, y)\n",
    "            epoch_loss += loss.item()\n",
    "            # root_loss = torch.sqrt(loss)\n",
    "            \n",
    "            # backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step() # update model parameters\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(data_loader)  # Average loss per epoch\n",
    "        train_losses.append(avg_epoch_loss)  # Append average epoch loss\n",
    "        \n",
    "        validation_score = evaluate(network, X_test, y_test) # evaluation mode\n",
    "        validation_scores.append(validation_score)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}, validation score: {validation_score}')\n",
    "        network.train() # back to train mode\n",
    "\n",
    "        if validation_score < validation_score_best:\n",
    "            validation_score_best = validation_score\n",
    "            torch.save(network.state_dict(), file_name_model+'.pt') \n",
    "            \n",
    "    print(f'Best validation score:{validation_score_best}')\n",
    "    return validation_scores, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and train model\n",
    "full_model = LinearRegressionModel(train_trans.shape[1]-1, 1)\n",
    "full_model.load_state_dict(torch.load('full_model.pt'))\n",
    "# full_validation_scores, full_train_losses = train(full_model, train_trans, test_trans, 'full_model', n_epochs=15, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unaware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protected_attributes = ['Female', 'Male', 'White', 'Hispanic', 'Asian', 'Black', 'Other', 'Mexican', 'Puertorican', 'Amerindian']\n",
    "\n",
    "train_unaware = train_trans.drop(protected_attributes, axis=1)\n",
    "test_unaware = test_trans.drop(protected_attributes, axis=1)\n",
    "\n",
    "unaware_model = LinearRegressionModel(train_unaware.shape[1]-1, 1)\n",
    "unaware_model.load_state_dict(torch.load('unaware_model.pt'))\n",
    "# unaware_validation_scores, unaware_train_losses = train(unaware_model, train_unaware, test_unaware, 'unaware_model', n_epochs=15, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fair K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FairKModel(R, S,num_observations, GPA=None, LSAT=None, FYA=None):\n",
    "    num_race_cats = len(law_data['race'].unique())\n",
    "    num_sex_cats = len(law_data['sex'].unique())    \n",
    "\n",
    "    # 0,1 vectors for the normal distribution\n",
    "    # R and S are matrices\n",
    "    r0_vec = torch.zeros(num_race_cats)\n",
    "    s0_vec = torch.zeros(num_sex_cats)\n",
    "    r1_vec = torch.ones(num_race_cats)\n",
    "    s1_vec = torch.ones(num_sex_cats)\n",
    "\n",
    "    # prior latent variable 'K' (knowledge)\n",
    "    K = pyro.sample('K', dist.Normal(torch.tensor(0.), torch.tensor(1.)))\n",
    "\n",
    "    # priors for weights and baselines for GPA, LSAT and FYA\n",
    "    # GPA ~ N(b_G  + w_G^K K + w_G^R R + w_G^S S, \\sigma_G)\n",
    "    b_G = pyro.sample('b_G', dist.Normal(torch.tensor(0.), torch.tensor(1.)))\n",
    "    w_G_K = pyro.sample('w_G_K', dist.Normal(torch.tensor(0.), torch.tensor(1.)))\n",
    "    w_G_R = pyro.sample('w_G_R', dist.Normal(r0_vec,r1_vec)) # outputs a vec of normals of len(race)\n",
    "    w_G_S = pyro.sample('w_G_S', dist.Normal(s0_vec,s1_vec)) # outputs a vec of normals of len(sex)\n",
    "    sigma_G = pyro.sample('sigma_G', dist.InverseGamma(torch.tensor(1.), torch.tensor(1.)))\n",
    "\n",
    "    # LSAT ~ Poisson(exp( b_L +w_L^K K + w_L^R R + w_L^S S))\n",
    "    b_L = pyro.sample('b_L', dist.Normal(torch.tensor(0.), torch.tensor(1.)))\n",
    "    w_L_K = pyro.sample('w_L_K', dist.Normal(torch.tensor(0.), torch.tensor(1.)))\n",
    "    w_L_R = pyro.sample('w_L_R', dist.Normal(r0_vec,r1_vec))\n",
    "    w_L_S = pyro.sample('w_L_S', dist.Normal(s0_vec,s1_vec))\n",
    "\n",
    "    # FYA ~ N(w_F^K K + w_F^R R + w_F^S S, 1)\n",
    "    w_F_K = pyro.sample('w_F_K', dist.Normal(torch.tensor(0.), torch.tensor(1.)))\n",
    "    w_F_R = pyro.sample('w_F_R', dist.Normal(r0_vec,r1_vec))\n",
    "    w_F_S = pyro.sample('w_F_S', dist.Normal(s0_vec,s1_vec))\n",
    "\n",
    "    # Calculate the parameters values of the data generating distributions\n",
    "    # print(len(b_G))\n",
    "    # print(len(w_G_K * K))\n",
    "    # print((torch.matmul(w_G_R, R.transpose(0,1))).shape)\n",
    "    # # w 1 x num_col_race\n",
    "    # # R len_race x num_col_race\n",
    "    # print((torch.matmul(w_G_S, S)).shape)\n",
    "\n",
    "    mu_G = b_G + w_G_K * K + torch.matmul(w_G_R, R.transpose(0,1)) + torch.matmul(w_G_S, S.transpose(0,1))\n",
    "    lambda_L = b_L + w_L_K * K + torch.matmul(w_L_R, R.transpose(0,1)) + torch.matmul(w_L_S, S.transpose(0,1))\n",
    "    mu_F = w_F_K * K + torch.matmul(w_F_R, R.transpose(0,1)) + torch.matmul(w_F_S, S.transpose(0,1))\n",
    "\n",
    "    # sample observed data\n",
    "    # pyro.plate is to denote independent observations and for vectorized computation\n",
    "    # use if dealing with multiple observations\n",
    "    # num_observations ensures models LH calculations are performed across all datapoints\n",
    "    with pyro.plate('data', num_observations):\n",
    "        # gives likelihood of observed data given model parameters\n",
    "        gpa = pyro.sample('gpa', dist.Normal(mu_G, sigma_G), obs=GPA) # obs is observed\n",
    "        lsat = pyro.sample('lsat', dist.Poisson(lambda_L.exp()), obs=LSAT)\n",
    "        fya = pyro.sample('fya', dist.Normal(mu_F, torch.tensor(1.)), obs=FYA)\n",
    "\n",
    "    return gpa, lsat, fya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1444pt\" height=\"367pt\"\n",
       " viewBox=\"0.00 0.00 1443.95 367.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 363)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-363 1439.95,-363 1439.95,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_data</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"594.95,-8 594.95,-83 812.95,-83 812.95,-8 594.95,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"788.95\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n",
       "</g>\n",
       "<!-- K -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>K</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"789.95\" cy=\"-235\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"789.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">K</text>\n",
       "</g>\n",
       "<!-- gpa -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>gpa</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"630.95\" cy=\"-57\" rx=\"27.9\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"630.95\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">gpa</text>\n",
       "</g>\n",
       "<!-- K&#45;&gt;gpa -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>K&#45;&gt;gpa</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M789.75,-216.83C788.51,-190.52 782.3,-140.22 753.95,-111 725.95,-82.15 704.06,-100.65 667.95,-83 664.29,-81.21 660.58,-79.11 656.98,-76.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"658.68,-73.82 648.4,-71.26 654.84,-79.67 658.68,-73.82\"/>\n",
       "</g>\n",
       "<!-- lsat -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>lsat</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"704.95\" cy=\"-57\" rx=\"27.9\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"704.95\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">lsat</text>\n",
       "</g>\n",
       "<!-- K&#45;&gt;lsat -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>K&#45;&gt;lsat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M787.46,-216.72C783.36,-191.98 773.58,-145.65 753.95,-111 747.29,-99.25 737.55,-87.99 728.5,-78.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"730.78,-76.23 721.15,-71.81 725.92,-81.27 730.78,-76.23\"/>\n",
       "</g>\n",
       "<!-- fya -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>fya</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"777.95\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"777.95\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">fya</text>\n",
       "</g>\n",
       "<!-- K&#45;&gt;fya -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>K&#45;&gt;fya</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M788.77,-216.8C786.68,-186.1 782.29,-121.64 779.79,-84.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"783.28,-84.75 779.11,-75.01 776.29,-85.22 783.28,-84.75\"/>\n",
       "</g>\n",
       "<!-- b_G -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>b_G</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"246.95\" cy=\"-235\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"246.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">b_G</text>\n",
       "</g>\n",
       "<!-- b_G&#45;&gt;gpa -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>b_G&#45;&gt;gpa</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M246.73,-216.67C247.41,-189.67 252.97,-137.9 283.95,-111 329.52,-71.42 512.36,-61.36 592.73,-58.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"593.02,-62.33 602.91,-58.54 592.81,-55.33 593.02,-62.33\"/>\n",
       "</g>\n",
       "<!-- w_G_K -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>w_G_K</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"334.95\" cy=\"-235\" rx=\"41.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"334.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_G_K</text>\n",
       "</g>\n",
       "<!-- w_G_K&#45;&gt;gpa -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>w_G_K&#45;&gt;gpa</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.88,-216.76C340.84,-189.88 352.46,-138.27 384.95,-111 416.18,-84.78 532.7,-68.61 593.49,-61.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"593.9,-65.26 603.46,-60.69 593.13,-58.3 593.9,-65.26\"/>\n",
       "</g>\n",
       "<!-- w_G_R -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>w_G_R</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"435.95\" cy=\"-235\" rx=\"41.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"435.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_G_R</text>\n",
       "</g>\n",
       "<!-- w_G_R&#45;&gt;gpa -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>w_G_R&#45;&gt;gpa</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M438.42,-216.84C443.08,-190.56 455.59,-140.29 485.95,-111 515.45,-82.53 561.45,-69.16 593.71,-63.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"594.43,-66.44 603.67,-61.26 593.21,-59.55 594.43,-66.44\"/>\n",
       "</g>\n",
       "<!-- w_G_S -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>w_G_S</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"40.95\" cy=\"-235\" rx=\"40.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_G_S</text>\n",
       "</g>\n",
       "<!-- w_G_S&#45;&gt;gpa -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>w_G_S&#45;&gt;gpa</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M42.47,-216.75C45.88,-189.38 56.77,-136.51 90.95,-111 131.02,-81.08 476.23,-64.33 592.73,-59.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"593.11,-62.97 602.96,-59.07 592.82,-55.98 593.11,-62.97\"/>\n",
       "</g>\n",
       "<!-- sigma_G -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>sigma_G</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"149.95\" cy=\"-235\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"149.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">sigma_G</text>\n",
       "</g>\n",
       "<!-- sigma_G&#45;&gt;gpa -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>sigma_G&#45;&gt;gpa</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.84,-216.77C158.41,-189.44 173.39,-136.61 208.95,-111 269.89,-67.1 500.49,-59.5 592.6,-58.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"592.91,-61.73 602.87,-58.11 592.83,-54.73 592.91,-61.73\"/>\n",
       "</g>\n",
       "<!-- b_L -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>b_L</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"619.95\" cy=\"-235\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"619.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">b_L</text>\n",
       "</g>\n",
       "<!-- b_L&#45;&gt;lsat -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>b_L&#45;&gt;lsat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M622.43,-216.72C626.54,-191.98 636.31,-145.65 655.95,-111 662.61,-99.25 672.35,-87.99 681.39,-78.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"683.98,-81.27 688.75,-71.81 679.12,-76.23 683.98,-81.27\"/>\n",
       "</g>\n",
       "<!-- w_L_K -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>w_L_K</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"704.95\" cy=\"-235\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"704.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_L_K</text>\n",
       "</g>\n",
       "<!-- w_L_K&#45;&gt;lsat -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>w_L_K&#45;&gt;lsat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M704.95,-216.8C704.95,-186.23 704.95,-122.19 704.95,-85.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"708.45,-85.01 704.95,-75.01 701.45,-85.01 708.45,-85.01\"/>\n",
       "</g>\n",
       "<!-- w_L_R -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>w_L_R</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"874.95\" cy=\"-235\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"874.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_L_R</text>\n",
       "</g>\n",
       "<!-- w_L_R&#45;&gt;lsat -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>w_L_R&#45;&gt;lsat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M872.73,-216.74C868.43,-190.32 856.53,-139.88 825.95,-111 797.33,-83.98 777.24,-100.4 741.95,-83 738.3,-81.2 734.59,-79.09 731,-76.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"732.7,-73.8 722.42,-71.23 728.86,-79.65 732.7,-73.8\"/>\n",
       "</g>\n",
       "<!-- w_L_S -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>w_L_S</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"534.95\" cy=\"-235\" rx=\"39.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"534.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_L_S</text>\n",
       "</g>\n",
       "<!-- w_L_S&#45;&gt;lsat -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>w_L_S&#45;&gt;lsat</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M537.17,-216.74C541.46,-190.32 553.36,-139.88 583.95,-111 612.56,-83.98 632.65,-100.4 667.95,-83 671.6,-81.2 675.31,-79.09 678.9,-76.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"681.04,-79.65 687.48,-71.23 677.2,-73.8 681.04,-79.65\"/>\n",
       "</g>\n",
       "<!-- w_F_K -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>w_F_K</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"1171.95\" cy=\"-235\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1171.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_F_K</text>\n",
       "</g>\n",
       "<!-- w_F_K&#45;&gt;fya -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>w_F_K&#45;&gt;fya</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1170.26,-216.97C1166.62,-189.9 1155.4,-137.44 1121.95,-111 1074.97,-73.87 894.57,-62.47 815.48,-59.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"815.14,-55.69 805.01,-58.8 814.87,-62.69 815.14,-55.69\"/>\n",
       "</g>\n",
       "<!-- w_F_R -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>w_F_R</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"973.95\" cy=\"-235\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"973.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_F_R</text>\n",
       "</g>\n",
       "<!-- w_F_R&#45;&gt;fya -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>w_F_R&#45;&gt;fya</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M971.48,-216.84C966.83,-190.54 954.33,-140.26 923.95,-111 893.89,-82.06 846.82,-68.74 814.29,-62.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"814.72,-59.26 804.28,-61.03 813.55,-66.16 814.72,-59.26\"/>\n",
       "</g>\n",
       "<!-- w_F_S -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>w_F_S</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"1072.95\" cy=\"-235\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1072.95\" y=\"-231.3\" font-family=\"Times,serif\" font-size=\"14.00\">w_F_S</text>\n",
       "</g>\n",
       "<!-- w_F_S&#45;&gt;fya -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>w_F_S&#45;&gt;fya</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1071.01,-216.76C1067.04,-189.89 1055.42,-138.29 1022.95,-111 991.75,-84.79 875.11,-68.56 814.71,-61.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"814.79,-58.23 804.47,-60.61 814.03,-65.19 814.79,-58.23\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-343.8\" font-family=\"Times,serif\" font-size=\"14.00\">K ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-328.8\" font-family=\"Times,serif\" font-size=\"14.00\">b_G ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-313.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_G_K ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-298.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_G_R ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-283.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_G_S ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-268.8\" font-family=\"Times,serif\" font-size=\"14.00\">sigma_G ~ InverseGamma</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-253.8\" font-family=\"Times,serif\" font-size=\"14.00\">b_L ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-238.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_L_K ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-223.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_L_R ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_L_S ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-193.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_F_K ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-178.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_F_R ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-163.8\" font-family=\"Times,serif\" font-size=\"14.00\">w_F_S ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\">gpa ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\">lsat ~ Poisson</text>\n",
       "<text text-anchor=\"start\" x=\"1237.95\" y=\"-118.8\" font-family=\"Times,serif\" font-size=\"14.00\">fya ~ Normal</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fe36a61c7d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor = torch.tensor(train_trans.values, dtype=torch.float32)\n",
    "\n",
    "# structure of FairKModel\n",
    "# generates a DAG of the model, showing how different rvs are related to each other within the model\n",
    "model_graph = pyro.render_model(\n",
    "    FairKModel, \n",
    "    model_args=(train_tensor[:,5:], train_tensor[:,3:5], train_tensor.shape[0], train_tensor[:,1], train_tensor[:,0], train_tensor[:,2]),\n",
    "    render_distributions=True, \n",
    "    render_params=True\n",
    "    )\n",
    "model_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduces background latend variables, U, which are not decendants of protected demographic factors. Information about X is passed to $\\hat{Y}$ via $P(U|x,a)$\n",
    "\n",
    "If we can't calculate P_M(U|x,a) analytically use the following algorithm:\n",
    "\n",
    "START Procedure FAIRLEARNING(D,M)\n",
    "1. For each datapoint $i \\in D$, sample $m$ MCMC samples U_1^{(i)}, \\cdots, U_1^{(i)} ~ P_M(U | x^{(i)}, a^{(i)})\n",
    "2. Let D’ be the augmented dataset where each point (a^{(i)}, x^{(i)}, y^{(i)}) in D is replaced with the corresponding m points \\{ (a^{(i)}, x^{(i)}, y^{(i)}, u_j^{(i)}) \\}\n",
    "3. \\hat{\\theta} \\leftarrow argmin_\\theta \\sum_{i’ \\in D’} l(y^{(i’)}, g_\\theta(U^{(i’)}, x^{(i’)}_{\\\\ A}))\n",
    "\n",
    "END procedure\n",
    "\n",
    "\n",
    "In Bayesian Terms for our scenario:\n",
    "\n",
    "P(K∣GPA,LSAT,FYA,Race,Sex) is the posterior distribution of K.\n",
    "\n",
    "P(GPA,LSAT,FYA∣K,Race,Sex) is the likelihood of observing the data given K.\n",
    "\n",
    "P(K) is the prior distribution of K, representing our beliefs about K before observing the data.\n",
    "\n",
    "P(GPA,LSAT,FYA,Race,Sex) is the evidence, or the probability of observing the data under all possible values of K.\n",
    "\n",
    "We can then sample from our posterior using MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853997e2a570418e91e6bcbc8897cb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:   0%|          | 0/1000 [1:37:35, ?it/s]\n",
      "Warmup:  20%|█▉        | 196/1000 [1:35:31, 29.24s/it, step size=5.72e-03, acc. prob=0.779]\n",
      "Warmup:  27%|██▋       | 398/1500 [1:14:18, 11.20s/it, step size=1.05e-07, acc. prob=0.764]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "K_list = []\n",
    "for i in tqdm(range(train_tensor.shape[0])):\n",
    "        # P(GPA,LSAT,FYA∣K,Race,Sex), so gpa, lsat and fya conditioned on observed data from train_tensor\n",
    "        conditioned_model = pyro.condition(FairKModel, data={\n",
    "                'gpa': train_tensor[i, 1], \n",
    "                'lsat': train_tensor[i, 0].type(torch.int32), \n",
    "                'fya': train_tensor[i, 2]})\n",
    "\n",
    "        # MCMC is too expensive to run\n",
    "        # initialize No-U-Turn Sampler a type of mcmc method\n",
    "        # nuts_kernel = pyro.infer.mcmc.NUTS(conditioned_model)\n",
    "        # sets up mcmc process using nuts kernel. draws 500 samples from the posterior and uses 500 iterations to tune the sampler\n",
    "        # mcmc = pyro.infer.MCMC(nuts_kernel, num_samples=500, warmup_steps=200, num_chains= os.cpu_count() // 2)\n",
    "\n",
    "        # Imporance sampling\n",
    "        importance = pyro.infer.Importance(conditioned_model, num_samples=10)\n",
    "\n",
    "        # executes the mcmc process\n",
    "        importance.run(R=train_tensor[:,5:], S=train_tensor[:,3:5], num_observations=train_tensor.shape[0]) # samples from P_M(U | x^{(i)}, a^{(i)})\n",
    "\n",
    "        # obtains distribution of sampled values for K\n",
    "        marginal = pyro.infer.EmpiricalMarginal(importance, sites=\"K\")\n",
    "        K_list.append(marginal.mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
